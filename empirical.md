# Emipirical Study and Survey on Long-context LLMs
*Here're some resources about Empirical Study and Survey on Long-context LLMs*

### Emipirical Study


#### Lost in the Middle- How Language Models Use Long Contexts

paper link: [here](https://arxiv.org/pdf/2307.03172.pdf)

citation:

```bibtex
@misc{liu2023lost,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


#### Sharp nearby, fuzzy far away: How neural language models use context

paper link: [here](https://arxiv.org/pdf/1805.04623)

citation:

```bibtex
@article{khandelwal2018sharp,
  title={Sharp nearby, fuzzy far away: How neural language models use context},
  author={Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1805.04623},
  year={2018}
}
```


### Survey

#### A survey on long text modeling with transformers

paper link: [here](https://arxiv.org/pdf/2302.14502)

citation:

```bibtex
@article{dong2023survey,
  title={A survey on long text modeling with transformers},
  author={Dong, Zican and Tang, Tianyi and Li, Lunyi and Zhao, Wayne Xin},
  journal={arXiv preprint arXiv:2302.14502},
  year={2023}
}
```


#### An empirical survey on long document summarization: Datasets, models, and metrics

paper link: [here](https://arxiv.org/pdf/2207.00939)

citation:

```bibtex
@article{koh2022empirical,
  title={An empirical survey on long document summarization: Datasets, models, and metrics},
  author={Koh, Huan Yee and Ju, Jiaxin and Liu, Ming and Pan, Shirui},
  journal={ACM computing surveys},
  volume={55},
  number={8},
  pages={1--35},
  year={2022},
  publisher={ACM New York, NY}
}
```


#### A survey of transformers

paper link: [here](https://www.sciencedirect.com/science/article/pii/S2666651022000146)

citation:

```bibtex
@article{lin2022survey,
  title={A survey of transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={AI Open},
  year={2022},
  publisher={Elsevier}
}
```

#### Long-context transformers: A survey

paper link: [here](https://ieeexplore.ieee.org/abstract/document/9587279/)

citation:

```bibtex
@inproceedings{ziyaden2021long,
  title={Long-context transformers: A survey},
  author={Ziyaden, Atabay and Yelenov, Amir and Pak, Alexandr},
  booktitle={2021 5th Scientific School Dynamics of Complex Networks and their Applications (DCNA)},
  pages={215--218},
  year={2021},
  organization={IEEE}
}
```


#### Efficient Transformers: A Survey

paper link: [here](https://arxiv.org/pdf/2009.06732.pdf)

citation:

```bibtex
@misc{tay2022efficient,
      title={Efficient Transformers: A Survey}, 
      author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
      year={2022},
      eprint={2009.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


