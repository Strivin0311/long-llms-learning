# checkpointing
*Here're some resources about gradient checkpointing strategies to make LLMs training more memory-efficient*


#### Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2306.00477)

citation: 
```bibtex
@article{liao2023make,
  title={Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning},
  author={Liao, Baohao and Tan, Shaomu and Monz, Christof},
  journal={arXiv preprint arXiv:2306.00477},
  year={2023}
}
```


#### Reformer: The efficient transformer [`READ`]

paper link: [here](https://arxiv.org/pdf/2001.04451)

citation: 
```bibtex
@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}
```



### References


#### The reversible residual network: Backpropagation without storing activations (RevNet) [`UNREAD`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2017/file/f9be311e65d81a9ad8150a60844bb94c-Paper.pdf)

citation: 
```bibtex
@article{gomez2017reversible,
  title={The reversible residual network: Backpropagation without storing activations},
  author={Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
```


#### Training deep nets with sublinear memory cost [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/1604.06174)

citation: 
```bibtex
@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}
```
    







