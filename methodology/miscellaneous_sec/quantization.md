# Quantization
*Here's some resources about Quantization for LLMs training and inference, deployment*


#### QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2309.14717)

citation: 
```bibtex
@article{xu2023qa,
  title={QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models},
  author={Xu, Yuhui and Xie, Lingxi and Gu, Xiaotao and Chen, Xin and Chang, Heng and Zhang, Hengheng and Chen, Zhensu and Zhang, Xiaopeng and Tian, Qi},
  journal={arXiv preprint arXiv:2309.14717},
  year={2023}
}
```

#### Qlora: Efficient finetuning of quantized llms [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2305.14314)

citation: 
```bibtex
@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}
```


#### Llm. int8 (): 8-bit matrix multiplication for transformers at scale [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2208.07339.pdf?trk=public_post_comment-text)

citation: 
```bibtex
@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}
```


#### Q8bert: Quantized 8bit bert [`READ`]

paper link: [here](https://arxiv.org/pdf/1910.06188)

citation: 
```bibtex
@inproceedings{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  booktitle={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)},
  pages={36--39},
  year={2019},
  organization={IEEE}
}
```
    
    