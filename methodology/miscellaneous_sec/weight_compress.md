# Weight Compression
*Here're some resources about Weight Compression strategies for LLMs*


### Table of Contents

* [Weight Quantization]()
* [Weight Pruning]()
* [Weight Factorization]()
* [Weight Partitioning]()
* [Weight Distillation]()


### Weight Quantization

*For the resourses, Please redirect to the section in another github repo: [here](https://github.com/Strivin0311/llms-learning/blob/main/modeling/weight-compress/quantization.md).*


### Weight Pruning

*For the resourses, Please redirect to the section in another github repo: [here](https://github.com/Strivin0311/llms-learning/blob/main/modeling/weight-compress/pruning.md).*


### Weight Factorization


#### Albert: A lite bert for self-supervised learning of language representations

paper link: [here](https://arxiv.org/pdf/1909.11942.pdf)

citation:

```bibtex
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}
```


### Weight Partitioning


####  Efficiently scaling transformer inference

paper link: [here](https://proceedings.mlsys.org/paper_files/paper/2023/file/523f87e9d08e6071a3bbd150e6da40fb-Paper-mlsys2023.pdf)

citation:

```bibtex
@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}
```


### Weight Distillation


#### Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers

paper link: [here](https://proceedings.neurips.cc/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

citation:

```bibtex
@article{wang2020minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020}
}
```


