# Specific Objectives
*Here're some resources about Specific Objectives*


#### PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization (Entity Pyramid) [`READ`]

paper link: [here](https://arxiv.org/pdf/2110.08499)

citation:

```bibtex
@article{xiao2021primera,
  title={PRIMERA: Pyramid-based masked sentence pre-training for multi-document summarization},
  author={Xiao, Wen and Beltagy, Iz and Carenini, Giuseppe and Cohan, Arman},
  journal={arXiv preprint arXiv:2110.08499},
  year={2021}
}
```


#### ERNIE-Doc: A retrospective long-document modeling transformer (Segment-Reordering) [`READ`]

paper link: [here](https://arxiv.org/pdf/2012.15688)

citation:

```bibtex
@article{ding2020ernie,
  title={ERNIE-Doc: A retrospective long-document modeling transformer},
  author={Ding, Siyu and Shang, Junyuan and Wang, Shuohuan and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2012.15688},
  year={2020}
}
```


#### Pegasus: Pre-training with extracted gap-sentences for abstractive summarization (GSG) [`READ`]

paper link: [here](http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf)

citation:

```bibtex
@inproceedings{zhang2020pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}
```


#### A divide-and-conquer approach to the summarization of long documents (DANCE) [`READ`]

paper link: [here](https://arxiv.org/pdf/2004.06190)

citation:

```bibtex
@article{gidiotis2020divide,
  title={A divide-and-conquer approach to the summarization of long documents},
  author={Gidiotis, Alexios and Tsoumakas, Grigorios},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={3029--3040},
  year={2020},
  publisher={IEEE}
}
```


#### Xlnet: Generalized autoregressive pretraining for language understanding [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)

citation:

```bibtex
@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
```