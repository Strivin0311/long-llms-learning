# Parallelism
*Here's some resources about Parallelism for LLMs training and inference*



#### Pytorch FSDP: experiences on scaling fully sharded data parallel [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2304.11277)

citation: 
```bibtex
@article{zhao2023pytorch,
  title={Pytorch FSDP: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}
```


#### Colossal-ai: A unified deep learning system for large-scale parallel training (Auto Parallel) [`UNREAD`]

paper link: [here](https://arxiv.org/pdf/2110.14883)

citation: 
```bibtex
@inproceedings{li2023colossal,
  title={Colossal-ai: A unified deep learning system for large-scale parallel training},
  author={Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},
  booktitle={Proceedings of the 52nd International Conference on Parallel Processing},
  pages={766--775},
  year={2023}
}
```
    