# Hierarchical Attention
*Here're some resources about Hierarchical Attention*



#### Hegel: Hypergraph transformer for long document summarization [`READ`]

paper link: [here](https://arxiv.org/pdf/2210.04126)

citation: 
```bibtex
@article{zhang2022hegel,
  title={Hegel: Hypergraph transformer for long document summarization},
  author={Zhang, Haopeng and Liu, Xiao and Zhang, Jiawei},
  journal={arXiv preprint arXiv:2210.04126},
  year={2022}
}
```
    

#### H-transformer-1d: Fast one-dimensional hierarchical attention for sequences [`READ`]

paper link: [here](https://arxiv.org/pdf/2107.11906)

citation: 
```bibtex
@article{zhu2021h,
  title={H-transformer-1d: Fast one-dimensional hierarchical attention for sequences},
  author={Zhu, Zhenhai and Soricut, Radu},
  journal={arXiv preprint arXiv:2107.11906},
  year={2021}
}
```
    
#### Hierarchical learning for generation with long source sequences [`READ`]

paper link: [here](https://arxiv.org/pdf/2104.07545)

citation: 
```bibtex
@article{rohde2021hierarchical,
  title={Hierarchical learning for generation with long source sequences},
  author={Rohde, Tobias and Wu, Xiaoxia and Liu, Yinhan},
  journal={arXiv preprint arXiv:2104.07545},
  year={2021}
}
```

#### Lite transformer with long-short range attention [`READ`]

paper link: [here](https://arxiv.org/pdf/2004.11886)

citation: 
```bibtex
@article{wu2020lite,
  title={Lite transformer with long-short range attention},
  author={Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  journal={arXiv preprint arXiv:2004.11886},
  year={2020}
}
```
        

#### Adaptive attention span in transformers [`READ`]

paper link: [here](https://arxiv.org/pdf/1905.07799)

citation: 
```bibtex
@article{sukhbaatar2019adaptive,
  title={Adaptive attention span in transformers},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:1905.07799},
  year={2019}
}
```
    
#### Hierarchical transformers for long document classification [`READ`]

paper link: [here](https://arxiv.org/pdf/1910.10781)

citation: 
```bibtex
@inproceedings{pappagari2019hierarchical,
  title={Hierarchical transformers for long document classification},
  author={Pappagari, Raghavendra and Zelasko, Piotr and Villalba, Jes{\'u}s and Carmiel, Yishay and Dehak, Najim},
  booktitle={2019 IEEE automatic speech recognition and understanding workshop (ASRU)},
  pages={838--844},
  year={2019},
  organization={IEEE}
}
```

#### A discourse-aware attention model for abstractive summarization of long documents [`READ`]

paper link: [here](https://arxiv.org/pdf/1804.05685)

citation: 
```bibtex
@article{cohan2018discourse,
  title={A discourse-aware attention model for abstractive summarization of long documents},
  author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  journal={arXiv preprint arXiv:1804.05685},
  year={2018}
}
```
    
    
