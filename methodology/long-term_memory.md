# Long-Term Memory
*Here's some resources about Long-Term Memory*



## Taxonomy

### Internal MemoryCache

#### Scaling Transformer to 1M tokens and beyond with RMT [`READ`]

paper link: [here](https://arxiv.org/pdf/2304.11062.pdf??ref=eiai.info)

citation: 
```bibtex
@article{bulatov2023scaling,
  title={Scaling Transformer to 1M tokens and beyond with RMT},
  author={Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S},
  journal={arXiv preprint arXiv:2304.11062},
  year={2023}
}
```


#### Improving language models by retrieving from trillions of tokens (RETRO) [`READ`]

paper link: [here](https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf)

citation: 
```bibtex
@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}
```

#### Memorizing transformers [`READ`]

paper link: [here](https://arxiv.org/pdf/2203.08913)

citation: 
```bibtex
@article{wu2022memorizing,
  title={Memorizing transformers},
  author={Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal={arXiv preprint arXiv:2203.08913},
  year={2022}
}
```

#### âˆž-former: Infinite Memory Transformer [`READ`]

paper link: [here](https://arxiv.org/pdf/2109.00301)

citation: 
```bibtex
@article{martins2021infty,
  title={$$\backslash$infty $-former: Infinite Memory Transformer},
  author={Martins, Pedro Henrique and Marinho, Zita and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2109.00301},
  year={2021}
}
```

#### Readtwice: Reading very large documents with memories [`READ`]

paper link: [here](https://arxiv.org/pdf/2105.04241)

citation: 
```bibtex
@article{zemlyanskiy2021readtwice,
  title={Readtwice: Reading very large documents with memories},
  author={Zemlyanskiy, Yury and Ainslie, Joshua and de Jong, Michiel and Pham, Philip and Eckstein, Ilya and Sha, Fei},
  journal={arXiv preprint arXiv:2105.04241},
  year={2021}
}
```
    
    
#### ERNIE-Doc: A retrospective long-document modeling transformer [`READ`]

paper link: [here](https://arxiv.org/pdf/2012.15688)

citation: 
```bibtex
@article{ding2020ernie,
  title={ERNIE-Doc: A retrospective long-document modeling transformer},
  author={Ding, Siyu and Shang, Junyuan and Wang, Shuohuan and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2012.15688},
  year={2020}
}
```

#### Memformer: A memory-augmented transformer for sequence modeling [`READ`]

paper link: [here](https://arxiv.org/pdf/2010.06891)

citation: 
```bibtex
@article{wu2020memformer,
  title={Memformer: A memory-augmented transformer for sequence modeling},
  author={Wu, Qingyang and Lan, Zhenzhong and Qian, Kun and Gu, Jing and Geramifard, Alborz and Yu, Zhou},
  journal={arXiv preprint arXiv:2010.06891},
  year={2020}
}
```


#### Compressive transformers for long-range sequence modelling [`READ`]

paper link: [here](https://arxiv.org/pdf/1911.05507)

citation: 
```bibtex
@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}
```

#### Transformer-xl: Attentive language models beyond a fixed-length context [`READ`]

paper link: [here](https://arxiv.org/pdf/1901.02860.pdf%3Ffbclid%3DIwAR3nwzQA7VyD36J6u8nEOatG0CeW4FwEU_upvvrgXSES1f0Kd-)

citation: 
```bibtex
@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}
```
    
    

### External MemoryBank

#### Creating large language model applications utilizing langchain: A primer on developing llm apps fast [`READ`]

paper link: [here](https://www.researchgate.net/profile/Oguzhan-Topsakal/publication/372669736_Creating_Large_Language_Model_Applications_Utilizing_LangChain_A_Primer_on_Developing_LLM_Apps_Fast/links/64d114a840a524707ba4a419/Creating-Large-Language-Model-Applications-Utilizing-LangChain-A-Primer-on-Developing-LLM-Apps-Fast.pdf)

citation: 
```bibtex
@inproceedings{topsakal2023creating,
  title={Creating large language model applications utilizing langchain: A primer on developing llm apps fast},
  author={Topsakal, Oguzhan and Akinci, Tahir Cetin},
  booktitle={Proceedings of the International Conference on Applied Engineering and Natural Sciences, Konya, Turkey},
  pages={10--12},
  year={2023}
}
```

#### RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.13304)

citation: 
```bibtex
@article{zhou2023recurrentgpt,
  title={RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text},
  author={Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cui, Peng and Wang, Tiannan and Xiao, Zhenxin and Hou, Yifan and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2305.13304},
  year={2023}
}
```
    

#### MemoryBank: Enhancing Large Language Models with Long-Term Memory [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.10250)

citation: 
```bibtex
@article{zhong2023memorybank,
  title={MemoryBank: Enhancing Large Language Models with Long-Term Memory},
  author={Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Wang, Yanlin},
  journal={arXiv preprint arXiv:2305.10250},
  year={2023}
}
```
    
#### RecallM: An Architecture for Temporal Context Understanding and Question Answering [`READ`]

paper link: [here](https://arxiv.org/pdf/2307.02738)

citation: 
```bibtex
@article{kynoch2023recallm,
  title={RecallM: An Architecture for Temporal Context Understanding and Question Answering},
  author={Kynoch, Brandon and Latapie, Hugo},
  journal={arXiv preprint arXiv:2307.02738},
  year={2023}
}
```

#### RET-LLM: Towards a General Read-Write Memory for Large Language Models [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.14322)

citation: 
```bibtex
@article{modarressi2023ret,
  title={RET-LLM: Towards a General Read-Write Memory for Large Language Models},
  author={Modarressi, Ali and Imani, Ayyoob and Fayyaz, Mohsen and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2305.14322},
  year={2023}
}
```
    
    

#### Augmenting Language Models with Long-Term Memory (LongMem) [`READ`]

paper link: [here](https://arxiv.org/pdf/2306.07174)

citation: 
```bibtex
@article{wang2023augmenting,
  title={Augmenting Language Models with Long-Term Memory},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  journal={arXiv preprint arXiv:2306.07174},
  year={2023}
}
```

#### REALM: Retrieval augmented language model pre-training [`READ`]

paper link: [here](http://proceedings.mlr.press/v119/guu20a/guu20a.pdf)

citation: 
```bibtex
@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}
```
    