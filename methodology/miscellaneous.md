# Miscellaneous Methodology for long LLMs
*Here're some resources about Miscellaneous Methodology for long LLMs*


### Intro

This section provides a concise overview of miscellaneous solutions that extend the previously discussed four categories, offering a broader perspective on enhancing the effective context window of LLMs or optimizing the efficiency when using off-the-shelf LLMs. It is worth noting that the literature covered here may not be exhaustive or specific to Transformer-based models. Many of these techniques are applicable universally to any model equipped with deep neural networks, albeit particularly crucial for large-scale LLMs.


### Table of Contents
* [Intro](#intro)
* [Specific Objectives](./miscellaneous_sec/spec_objective.md)
* [Specific Activation Functions](./miscellaneous_sec/spec_activation.md)
* [Mixture of Experts](./miscellaneous_sec/moe.md)
* [Parallelism](./miscellaneous_sec/parallel.md)
* [Weight Compression](./miscellaneous_sec/weight_compress.md)
* [Long-contenxt Training](#long-contenxt-training)



### Long-contenxt Training


#### Data Engineering for Scaling Language Models to 128K Context [`READ`]

paper link: [here](https://arxiv.org/pdf/2402.10171.pdf)

citation:

```bibtex
@misc{fu2024data,
    title={Data Engineering for Scaling Language Models to 128K Context}, 
    author={Yao Fu and Rameswar Panda and Xinyao Niu and Xiang Yue and Hannaneh Hajishirzi and Yoon Kim and Hao Peng},
    year={2024},
    eprint={2402.10171},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```



    