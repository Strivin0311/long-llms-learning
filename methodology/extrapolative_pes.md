# Extrapolative PEs
*Here's some resources about Extrapolative PEs*



### Attention Bias


#### Dissecting transformer length extrapolation via the lens of receptive field analysis (Sandwitch) [`READ`]

paper link: [here](https://aclanthology.org/2023.acl-long.756.pdf)

citation: 
```bibtex
@inproceedings{chi2023dissecting,
  title={Dissecting transformer length extrapolation via the lens of receptive field analysis},
  author={Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander and Ramadge, Peter},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13522--13537},
  year={2023}
}
```

#### Kerple: Kernelized relative positional embedding for length extrapolation [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/37a413841a614b5414b333585e7613b8-Paper-Conference.pdf)

citation: 
```bibtex
@article{chi2022kerple,
  title={Kerple: Kernelized relative positional embedding for length extrapolation},
  author={Chi, Ta-Chung and Fan, Ting-Han and Ramadge, Peter J and Rudnicky, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8386--8399},
  year={2022}
}
```
    
    


#### Train short, test long: Attention with linear biases enables input length extrapolation [`READ`]

paper link: [here](https://arxiv.org/pdf/2108.12409.pdf%5C)

citation: 
```bibtex
@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}
```
    



### Extended RoPE

#### A Frustratingly Easy Improvement for Position Embeddings via Random Padding [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.04859)

citation: 
```bibtex
@article{tao2023frustratingly,
  title={A Frustratingly Easy Improvement for Position Embeddings via Random Padding},
  author={Tao, Mingxu and Feng, Yansong and Zhao, Dongyan},
  journal={arXiv preprint arXiv:2305.04859},
  year={2023}
}
```
    
#### Randomized Positional Encodings Boost Length Generalization of Transformers [`READ`]

paper link: [here](https://arxiv.org/pdf/2305.16843)

citation: 
```bibtex
@article{ruoss2023randomized,
  title={Randomized Positional Encodings Boost Length Generalization of Transformers},
  author={Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Csord{\'a}s, R{\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  journal={arXiv preprint arXiv:2305.16843},
  year={2023}
}
```
    

#### Extending context window of large language models via positional interpolation [`READ`]

paper link: [here](https://arxiv.org/pdf/2306.15595)

citation: 
```bibtex
@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}
```
    

#### A length-extrapolatable transformer (LEX) [`READ`]

paper link: [here](https://arxiv.org/pdf/2212.10554)

citation: 
```bibtex
@article{sun2022length,
  title={A length-extrapolatable transformer},
  author={Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10554},
  year={2022}
}
```

#### Permuteformer: Efficient relative position encoding for long sequences [`READ`]

paper link: [here](https://arxiv.org/pdf/2109.02377)

citation: 
```bibtex
@article{chen2021permuteformer,
  title={Permuteformer: Efficient relative position encoding for long sequences},
  author={Chen, Peng},
  journal={arXiv preprint arXiv:2109.02377},
  year={2021}
}
```

#### SHAPE: Shifted absolute position embedding for transformers [`READ`]

paper link: [here](https://arxiv.org/pdf/2109.05644)

citation: 
```bibtex
@article{kiyono2021shape,
  title={SHAPE: Shifted absolute position embedding for transformers},
  author={Kiyono, Shun and Kobayashi, Sosuke and Suzuki, Jun and Inui, Kentaro},
  journal={arXiv preprint arXiv:2109.05644},
  year={2021}
}
```



#### Roformer: Enhanced transformer with rotary position embedding [`READ`]

paper link: [here](https://arxiv.org/pdf/2104.09864)

citation: 
```bibtex
@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}
```

### Others

#### Exploring length generalization in large language models [`READ`]

paper link: [here](https://proceedings.neurips.cc/paper_files/paper/2022/file/fb7451e43f9c1c35b774bcfad7a5714b-Paper-Conference.pdf)

citation: 
```bibtex
@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}
```
    
    